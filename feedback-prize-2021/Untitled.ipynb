{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7021f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d4a80536",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(StringIO(\"\"\"id,class,predictionstring\n",
    "1,Claim,1 2 3 4 5\n",
    "1,Claim,6 7 8\n",
    "1,Claim,21 22 23 24 25\n",
    "1,Evidence,1 2 3 4 5\n",
    "1,Evidence,6 7 8\n",
    "1,Evidence,21 22 23 24 25\"\"\"))\n",
    "\n",
    "predictions = pd.read_csv(StringIO(\"\"\"id,class,predictionstring\n",
    "1,Claim,1 2\n",
    "1,Claim,6 7 8\n",
    "2,Claim,12\n",
    "1,Evidence,1 2 4 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fe0099c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>level_1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claim</td>\n",
       "      <td>false positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claim</td>\n",
       "      <td>true positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evidence</td>\n",
       "      <td>true positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Claim</td>\n",
       "      <td>false negative</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Evidence</td>\n",
       "      <td>false negative</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class         level_1  0\n",
       "0     Claim  false positive  2\n",
       "1     Claim   true positive  1\n",
       "2  Evidence   true positive  1\n",
       "3     Claim  false negative  2\n",
       "5  Evidence  false negative  2"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's first calculate true positives and false positives\n",
    "from functools import reduce\n",
    "\n",
    "def prediction_scores(group):\n",
    "    prediction = set(group.iloc[0][\"predictionstring_pred\"].split())\n",
    "    truth_sets = [set(g.split()) for i, g in group[\"predictionstring_truth\"].iteritems() if type(g) == str]\n",
    "    truth_scores = [(len(truth.intersection(prediction)) / max(len(prediction),len(truth))) >= 0.5 for truth in truth_sets ]\n",
    "    correct_prediction = max(truth_scores,default=False)\n",
    "    if correct_prediction:\n",
    "        return \"true positive\"\n",
    "    else:\n",
    "        return \"false positive\"\n",
    "    \n",
    "merged = predictions.merge(ground_truth, how=\"left\", on=[\"id\",\"class\"], suffixes=[\"_pred\",\"_truth\"])\n",
    "grouped = merged.groupby([\"id\",\"class\",\"predictionstring_pred\"])\n",
    "p = grouped.apply(prediction_scores).groupby(\"class\").value_counts()\n",
    "\n",
    "def truth_scores(group):\n",
    "    truth = set(group.iloc[0][\"predictionstring_truth\"].split())\n",
    "    prediction_sets = [set(g.split()) for i, g in group[\"predictionstring_pred\"].iteritems() if type(g) == str]\n",
    "    prediction_scores = [(len(truth.intersection(prediction)) / max(len(prediction),len(truth))) >= 0.5 for prediction in prediction_sets ]\n",
    "    correct_prediction = max(prediction_scores,default=False)\n",
    "    if correct_prediction:\n",
    "        return \"true positive\"\n",
    "    else:\n",
    "        return \"false negative\"\n",
    "    \n",
    "merged = predictions.merge(ground_truth, how=\"right\", on=[\"id\",\"class\"], suffixes=[\"_pred\",\"_truth\"])\n",
    "grouped = merged.groupby([\"id\",\"class\",\"predictionstring_truth\"])\n",
    "t = grouped.apply(truth_scores).groupby(\"class\").value_counts()\n",
    "\n",
    "counts = pd.DataFrame(p.append(t)).reset_index().drop_duplicates() # true positives should be in each one\n",
    "\n",
    "counts\n",
    "# now count the macro f3 score\n",
    "def f3_score(group):\n",
    "    \"\"\"Given a series with the counts by category, calculate the f3\"\"\"\n",
    "    return group.loc[\"true positive\"]\n",
    "    return group.get(\"true positive\",0)  / (\n",
    "        group.get(\"true positive\",0) + 0.1 * group.get(\"false positive\",0) + 0.9 * group.get(\"false negative\",0))\n",
    "#counts.groupby(\"class\").apply(f3_score)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffbf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
